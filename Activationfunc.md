# 深度学习中的Activation function 简介

---

> * 什么是激活函数
> * 为什么要用
> * sigmoid,Tanh,Relu,softmax的比较
> * Sigmoid 和 Softmax 区别

---

## 1.什么是激活函数

在神经元中，输入的inputs通过加权，求和后，还被作用了一个函数，就是激活函数 Activation function.

## 2.为什么要用

如果不用激励函数，每一层输出都是上层输入的线性函数，那么无论神经网络有多少层，输出都是输入的线性组合。没办法处理非线性数据。

如果使用的话，激活函数给神将元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中了。

## 3.sigmoid，Relu，softmax的比较

* sigmoid函数

  公式：f(x)=1/(1+exp(-x))

  用于隐层神经元输出，取值范围为(0,1),它可以将任意实数映射到该区间，可以用来做二分类。在特征相差比较复杂或者相差不是特别大的时候效果比较好。

  sigmoid缺点：

  激活函数计算量大，反向传播求误差梯度时，很容易出现梯度消失的情况，从而无法完成深层网络的训练。

  梯度消失原因：

  从sigmoid 的图像可以看出，当数据越大或越小时，sigmoid函数接近水平，导数为零，所以会出现梯度消失的情况。梯度消失指的是权重不再更新，直观上看是从最后一层到第一层权重的更新越来越慢，直至不更新。本质原因是反向传播的连乘效应，导致最后对权重的偏导接近于零。

* tanh

  公式：f(x)=tanh(x)=(exp(x)-exp(-x))/(exp(x)+exp(-x))

  取值范围是[-1,1],tanh在特征值相差明显时效果会很好，在循环过程中不断扩大特征效果，与sigmoid的区别是，tanh是0均值的，因此实际应用中会比sigmoid 函数更好。

* Relu

  公式： f(x)=max(0,x)

  ReLU 的优点：

  使用 ReLU 得到的 SGD 的收敛速度会比 sigmoid/tanh 快很多，不会出现梯度消失现象
  ReLU 的缺点：

  训练的时候很”脆弱”，很容易就”die”了

  例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。

* softmax

  softmax-用于多分类神经网络的输出

  softmax就是将神经元输出的每一个元素的每个类别的值求指数(保证所有值是正值)在除以这个元素所有类别求得的指数的和(求每个类别占总数的比例)

  例如一个神经层的输出(两个元素，三种类别)是：

  ```python
  输出
  [[2,6,2]
   [1,1,1]]
  softmax计算过程如下
  [[exp(2)/exp(2)+exp(6)+exp(2),exp(6)/exp(2)+exp(6)+exp(2),exp(2)/exp(2)+exp(6)+exp(2)],[e/3e.e/3e,e/3e]]
  ```

##4.Sigmoid 和 Softmax 区别

二分类问题时 sigmoid 和 softmax 是一样的，求的都是 cross entropy loss，而 softmax 可以用于多分类问题